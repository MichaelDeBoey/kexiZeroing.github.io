---
layout: "../layouts/BlogPost.astro"
title: "Whisper API turn audio into text"
slug: whisper-api-turn-audio-into-text
description: ""
added: "June 13 2023"
tags: [code]
updatedDate: "Dec 6 2023"
---

OpenAI Whisper is a transformer-based automatic speech recognition system with open source code. [Whisper](https://github.com/openai/whisper) is free to use, and the model is downloaded to your machine on the first run. You can try [whisper.cpp](https://github.com/ggerganov/whisper.cpp) and follow this post [AI auto-subtitling](https://www.spapas.net/2023/05/22/ai-auto-subtitling/). However, you also have an option of using the commercial API from OpenAI. Check the official guide at https://platform.openai.com/docs/guides/speech-to-text

For the real-time / streaming tasks, check out [this discussion page](https://github.com/openai/whisper/discussions/2) where are many speech2text apps built from the community.

```python
import openai, os

openai.api_key = os.getenv("OPENAI_API_KEY")

audio_file= open("/path/to/file/audio.mp3", "rb")
transcript = openai.Audio.transcribe("whisper-1", audio_file)

print(transcript['text'])s
```

> You can use a prompt to improve the quality of the transcripts generated by the Whisper API. The model will try to match the style of the prompt, so it will be more likely to use capitalization and punctuation if the prompt does too.

```python
audio_file= open("/path/to/file/audio.mp3", "rb")
transcript = openai.Audio.transcribe("whisper-1", audio_file, prompt="The transcript is about...")

print(transcript['text'])
```

```python
audio_file= open("/path/to/file/audio.mp3", "rb")
transcript = openai.Audio.transcribe("whisper-1", audio_file, response_format="srt", prompt="The transcript is about...")

print(transcript)
```

```python
audio_file= open("/path/to/file/audio.mp3", "rb")
transcript = openai.Audio.transcribe("whisper-1", audio_file, response_format="vtt", prompt="The transcript is about...")

print(transcript)
```

```python
# pip install pydub
# https://github.com/jiaaro/pydub

from pydub import AudioSegment

podcast = AudioSegment.from_mp3("./data/podcast_long.mp3")

ten_minutes = 10 * 60 * 1000
# len(audio_segment) returns milliseconds
total_length = len(podcast)

start = 0
index = 0
while start < total_length:
  end = start + ten_minutes
  if end < total_length:
    chunk = podcast[start:end]
  else:
    chunk = podcast[start:]
  with open(f"./data/podcast_clip_{index}.mp3", "wb") as f:
    # Save the results (again whatever ffmpeg supports)
    chunk.export(f, format="mp3")
  start = end
  index += 1


original_prompt = "The transcript is about...\n"
prompt = original_prompt

for i in range(index):
  clip = f"./data/podcast_clip_{i}.mp3"
  audio_file= open(clip, "rb")
  transcript = openai.Audio.transcribe("whisper-1", audio_file, prompt=prompt)
  
  # mkdir ./data/transcripts if not exists
  if not os.path.exists("./data/transcripts"):
    os.makedirs("./data/transcripts")
  # write to file
  with open(f"./data/transcripts/podcast_clip_{i}.txt", "w") as f:
    f.write(transcript['text'])

  # get last sentence of the transcript
  sentences = transcript['text'].split(".")
  prompt = original_prompt + sentences[-1] 

  # Summary the transcript using ChatGPT API
  completion = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
      {"role": "system", "content": "请你成为文章摘要的小帮手，摘要以下文字，以简体中文输出"},
      {"role": "user", "content": transcript}
    ]
  )

  print(completion.choices[0].message)
```

```python
import openai
from langchain.chat_models import ChatOpenAI
from dotenv import load_dotenv
import os
from langchain.prompts import PromptTemplate
from langchain import LLMChain

# Create a prompt template to be used in the chain.

template = """
    You are a management assistant who writes meeting minutes. You always manage to capture the important points.

    Below you will find a transcript of a recorded meeting.

    This report needs to be clearly and concisely written in English. Please conclude with action points at the bottom. Also, provide suggestions for topics to discuss in the next meeting.

    Transcript = {transcript}

    Response in markdown:

    """

prompt = PromptTemplate(
        input_variables=["transcript"],
        template=template,
    )

# Load env files
load_dotenv()
openai_api_key = os.environ.get('openai_api_key')

# Transcribe audio with Whisper API
audio_file_path = "path/to/your/audio/file"
transcript_raw = openai.Audio.transcribe("whisper-1", file=audio_file_path)

# Create LLM
llm = ChatOpenAI(openai_api_key=openai_api_key, model_name="gpt-3.5-turbo", temperature=0.3)

# Create prompt
prompt_with_transcript = prompt.format(transcript=str(transcript_raw))

# Create chain
chain = LLMChain(llm=llm, prompt=prompt_with_transcript)

# Run chain
summary = chain.run()
```

## Links

- Model Hub: https://huggingface.co/models?search=whisper

- Build real time speech2text web apps using OpenAI's Whisper: https://github.com/saharmor/whisper-playground

- An CLI to transcribe Audio files w/ Whisper on-device: https://github.com/Vaibhavs10/insanely-fast-whisper

- React Hook for OpenAI Whisper API with speech recorder and real-time transcription: https://github.com/chengsokdara/use-whisper

- Using the Web Speech API: https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API/Using_the_Web_Speech_API

- Web Speech API Demonstration: https://www.google.com/intl/en/chrome/demos/speech.html
